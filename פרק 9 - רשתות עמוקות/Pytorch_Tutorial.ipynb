{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch_Tutorial.ipynb","provenance":[],"collapsed_sections":["ChzhDpPPZwZ1","y0yYyIDi-DVy","maZnGuwo-bEu","ZOMMiUS8-iTA","c592nR1__vsZ","9lWfOt4J_3Va","rWwEpMST_7c6","6PFQSPImABS5","Ql8lYhQbP4L6"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-9-Pf1IcYAV8","colab_type":"text"},"source":["**Pytorch Tutorial**\n"]},{"cell_type":"code","metadata":{"id":"IkvGRUCaeodt","colab_type":"code","colab":{}},"source":["import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","import time\n","\n","import torch\n","import torchvision\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn import datasets\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ChzhDpPPZwZ1","colab_type":"text"},"source":["## Tensors"]},{"cell_type":"markdown","metadata":{"id":"Udcl8tCgZ6B1","colab_type":"text"},"source":["Tensors are the base data structures of PyTorch which are used for building different types of neural networks. They can be considered as the generalization of arrays and matrices; in other words, tensors are N-dimensional matrices or multidimensional arrays.\n","\n","PyTorch tensors are similar to NumPy’s n-dimensional arrays. But unlike ndarrays of numpy, these tensors can be stored on a GPU RAM as well as the CPU RAM, and than to be multiplied efficientlly on the small many cores of the GPU (this is not the case with NumPy arrays). This is a major advantage of using tensors."]},{"cell_type":"markdown","metadata":{"id":"34hjE_dnjuUE","colab_type":"text"},"source":["###  Tensor Initialization"]},{"cell_type":"markdown","metadata":{"id":"WVdMHgL5gsuy","colab_type":"text"},"source":["Tensors can be created by using a list:"]},{"cell_type":"code","metadata":{"id":"5gbGK5BifniP","colab_type":"code","colab":{}},"source":["x = torch.tensor([1,2,3])\n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hS2GJDPThBSg","colab_type":"text"},"source":["You can specify the data type of the tensor by using the ``dtype`` argument,\n","similarily to numpy, you can check the type of the tensor by using the ``dtype`` attribute:"]},{"cell_type":"code","metadata":{"id":"EWP-mViIiNrI","colab_type":"code","colab":{}},"source":["x = torch.tensor([1,2,3], dtype = torch.float32)\n","print(x)\n","print(x.dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hx3qL4M8n5eP","colab_type":"text"},"source":["It's important to note here that one way to evaluate a GPU is by its TFLOPS - the capability of a processor to calculate one trillion floating-point operations per second. These operations rate usually refer to a 32bit floating point and abiously depends on the tensot type. operations on a 64bit floating point will be half the rate and so on."]},{"cell_type":"markdown","metadata":{"id":"sjvJ5IaPkDl-","colab_type":"text"},"source":["Let’s say we want a matrix of shape 3*3 having all zeros/ones. Take a moment to think – how can we do that using NumPy?"]},{"cell_type":"code","metadata":{"id":"yXOFNYdaj4Bd","colab_type":"code","colab":{}},"source":["a = np.zeros((3,3))\n","b = np.ones((3,3))\n","\n","print(a)\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DY3HOvAOkUjU","colab_type":"text"},"source":["Similar to NumPy, PyTorch also has the zeros() and ones() function which takes the shape as input and returns a matrix of zeros or ones , respectively, of a specified shape."]},{"cell_type":"code","metadata":{"id":"NxZN_iT4j4FX","colab_type":"code","colab":{}},"source":["a = torch.zeros((3,3))\n","b = torch.ones((3,3))\n","\n","print(a)\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d8Y-nSZAoqBF","colab_type":"text"},"source":["The ``random.randn()`` function returns random numbers that follow a standard normal distribution. \n","\n","We can initialize a similar matrix of random numbers using PyTorch:\n","\n"]},{"cell_type":"code","metadata":{"id":"9syMOW-lj4I_","colab_type":"code","colab":{}},"source":["a = torch.randn(2,3)\n","\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vUMUuLeBDKfa","colab_type":"text"},"source":["### Tensor Copying"]},{"cell_type":"markdown","metadata":{"id":"GIej_VuCDP4Y","colab_type":"text"},"source":["As you will see later in this chapter,\n","most of the tensor operations create a view of the origin tensor.\n","\n","Since it is a room for errors you must notice when you need to create a copy of your origin tensor.\n","\n","Unlike numpy, in torch the syntax is a bit different."]},{"cell_type":"markdown","metadata":{"id":"4cOUs78lE5eb","colab_type":"text"},"source":["The function `copy_` copies the tensor given as a parameter."]},{"cell_type":"code","metadata":{"id":"PKPmKwwmCv2Q","colab_type":"code","colab":{}},"source":["a = torch.randn(3,3)\n","print(a, \"\\n\")\n","\n","b = torch.empty(3,3)\n","b.copy_(a)\n","print(b, \"\\n\")\n","\n","a[0,0] = 10\n","print(a[0,0], \"\\n\")\n","print(b[0,0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7bqdJJQWGwcL","colab_type":"text"},"source":["The function `clone` also copies a tensor.\n","\n","However, unlike `copy_`, this function is recorded in the computation graph. \n","(As you will see later in this tutorial, it means that gradients propagating to the cloned tensor will propagate to the original tensor.)"]},{"cell_type":"code","metadata":{"id":"J41DL91zGJHl","colab_type":"code","colab":{}},"source":["a = torch.randn(3,3)\n","print(a, \"\\n\")\n","\n","b = torch.empty(3,3)\n","b = a.clone()\n","print(b, \"\\n\")\n","\n","a[0,0] = 10\n","print(a[0,0], \"\\n\")\n","print(b[0,0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZE0e6D-D0x9t","colab_type":"text"},"source":["### Shaping and Reshaping"]},{"cell_type":"markdown","metadata":{"id":"okaAc0As07rF","colab_type":"text"},"source":["You can get the shape of a tensor using the ``.size()`` method or just the `.shape` attribute."]},{"cell_type":"code","metadata":{"id":"X9Ou2RMu0yOO","colab_type":"code","colab":{}},"source":["a = torch.randn(2,3)\n","\n","print(a.size())\n","print(a.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6zUy9uI22miZ","colab_type":"text"},"source":["We can use the `.view()` function and pass the required shape as a parameter. \n","\n","It will return a tensor with the new shape. \n","\n","Pay attention that the returned tensor isn't a copy of the original tensor, it will share the underling data with the original tensor.\n","\n","Let’s try to convert the above tensor of shape (2,3) to a tensor of shape (6,1):\n","\n"]},{"cell_type":"code","metadata":{"id":"xbEoYY0D2qPb","colab_type":"code","colab":{}},"source":["b = a.view(6,1)\n","print(b)\n","print(b.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HP7r3aoC2qSS","colab_type":"code","colab":{}},"source":["a[0][0]=5\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rzEu5hhq3ELO","colab_type":"text"},"source":["If you know you want, for example, 6 rows however, you want torch to conclude the number of columns, you should send -1 as the columns dim argument"]},{"cell_type":"code","metadata":{"id":"uIdpWk8R3dlD","colab_type":"code","colab":{}},"source":["a = torch.randn(3,4,5)\n","b = a.view(-1,12)\n","print(b)\n","print(b.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dl8K1RR2IAVh","colab_type":"text"},"source":["If you have two twnsors with the same amount of elements and you want to make their shape the same you can pass one's shape as a parameter to the `view` function"]},{"cell_type":"code","metadata":{"id":"PD_ddmzuH3BL","colab_type":"code","colab":{}},"source":["a = torch.randn(3,4,5)\n","print(a.size())\n","b = torch.randn(6, 10)\n","print(b.size(), \"\\n\")\n","\n","b = b.view(a.size())\n","print(b.size())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B6QZ5OaUIx4e","colab_type":"text"},"source":["However, you can do it more elegantlly using the `view_as` function:"]},{"cell_type":"code","metadata":{"id":"5VPj_AWvI-mT","colab_type":"code","colab":{}},"source":["a = torch.randn(3,4,5)\n","print(a.size())\n","b = torch.randn(6, 10)\n","print(b.size(), \"\\n\")\n","\n","b = b.view_as(a)\n","print(b.size())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4W2-j4da2-rJ","colab_type":"text"},"source":["You can also use the `torch.reshape` method,\n","however, pay attentaion that torch.reshape may return a copy or a view of the original tensor - `torch.reshape` method will try to share the memory of the returned tensor, but if that complicates things for the torch architecture torch will make a clone of it and will return a new reshaped tensor. Thus you can not count on that to return a view or a copy."]},{"cell_type":"markdown","metadata":{"id":"VkoyXPWYT5ek","colab_type":"text"},"source":["In order to transpose a 2D tensor, you can use ``torch.t`` as follows:"]},{"cell_type":"code","metadata":{"id":"_f8AKKPaT9Ou","colab_type":"code","colab":{}},"source":["a = torch.randn(2,3)\n","\n","print(a)\n","print(f\"tensor a shape is {a.shape} \\n\")\n","\n","a_t = torch.t(a)\n","print(a_t)\n","print(f\"the transpose of tensor a shape is {a_t.shape} \\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-j1Y7aG1UH5J","colab_type":"text"},"source":["Pay attention that 0-D and 1-D tensors are returned as is. "]},{"cell_type":"code","metadata":{"id":"obxGtUGjUJGS","colab_type":"code","colab":{}},"source":["a = torch.randn(5)\n","print(a.shape, \"\\n\")\n","\n","a_t = torch.t(a)\n","print(a_t.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ASK30QiE1Jh7","colab_type":"text"},"source":["If you want to transpose a tensor with more than 2 dimensions, you can use the `permute` function:"]},{"cell_type":"code","metadata":{"id":"TEB798Wf2uxu","colab_type":"code","colab":{}},"source":["a = torch.randn(3, 4, 5)\n","print(a.size())\n","b.copy_(a.permute(2, 0, 1))\n","print(b.size())\n","\n","print(a[1,2,3] == b[3,1,2])\n","print(a[1,2,3] == b[1,2,3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OyY_of_VtjKr","colab_type":"text"},"source":["### Mathematical Operations"]},{"cell_type":"markdown","metadata":{"id":"PQNGI21FuNrB","colab_type":"text"},"source":["Let’s now see how we can do mathematical operations using PyTorch on tensors. So, first, let’s initialize two tensors:"]},{"cell_type":"code","metadata":{"id":"Y8YW3fnztiq7","colab_type":"code","colab":{}},"source":["a = torch.randn(2,3)\n","b = torch.randn(2,3)\n","\n","print(a)\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xEJJRX5QvRm8","colab_type":"text"},"source":["There are 3 different ways to perform mathematical operations:"]},{"cell_type":"markdown","metadata":{"id":"aWOf2t7Evbzs","colab_type":"text"},"source":["1. Well known  mathematical operators:"]},{"cell_type":"code","metadata":{"id":"788AXVMbj4OU","colab_type":"code","colab":{}},"source":["# addition\n","print(a+b, \"\\n\")       # equivalent to: torch.add(a, b)\n","\n","# subtraction\n","print(b-a, \"\\n\")       # equivalent to: torch.sub(b, a)\n","\n","# elementwise multiplication\n","print(a*b, \"\\n\")       # equivalent to: torch.mul(a, b)\n","\n","# dot product\n","print(a@torch.t(b), \"\\n\")       # equivalent to: torch.dot(a, b)\n","\n","# division\n","print(b/a, \"\\n\")       # equivalent to: torch.div(b, a)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZugTYV7xGC_","colab_type":"text"},"source":["2. Torch methods"]},{"cell_type":"code","metadata":{"id":"FqzSCIVrj4T7","colab_type":"code","colab":{}},"source":["# addition\n","print(torch.add(a, b), \"\\n\")\n","\n","# subtraction\n","print(torch.sub(b, a), \"\\n\")\n","\n","# elementwise multiplication\n","print(torch.mul(a, b), \"\\n\")\n","\n","# dot product\n","print(torch.mm(a, torch.t(b)), \"\\n\")\n","\n","# division\n","print(torch.div(b, a), \"\\n\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"shsKUNJ7yeAD","colab_type":"text"},"source":["3. Torch inplace methods\n","\n","  The inplace methods functionality is the same as the regular methods shown above, accept that their reult is written to the object they were operated on.\n","The inplace methods always followed by an underscore."]},{"cell_type":"code","metadata":{"id":"4cbKzQeXj4Sc","colab_type":"code","colab":{}},"source":["# addition\n","a.add_(b)\n","print(a, \"\\n\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l0SU3KKV0gCn","colab_type":"text"},"source":["### Concatenating"]},{"cell_type":"markdown","metadata":{"id":"mCbZnCFANYZu","colab_type":"text"},"source":["Let’s say we have two tensors as shown below and we want to concatenate them\n"]},{"cell_type":"code","metadata":{"id":"eZJ3PbNfM2ts","colab_type":"code","colab":{}},"source":["a = torch.tensor([[1,2],[3,4]])\n","b = torch.tensor([[5,6],[7,8]])\n","\n","print(a, \"\\n\")\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lrG7ypNSONrb","colab_type":"code","colab":{}},"source":["# concatenating vertically\n","torch.cat((a,b))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AO2QeXW8OMzo","colab_type":"text"},"source":["As you can see, the second tensor has been stacked below the first tensor. We can concatenate the tensors horizontally as well by setting the dim parameter to 1:\n"]},{"cell_type":"code","metadata":{"id":"wrcdmLMNONvX","colab_type":"code","colab":{}},"source":["# concatenating horizontally\n","torch.cat((a,b),dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ylevJwi7IQS","colab_type":"text"},"source":["### Accumulating Functions"]},{"cell_type":"markdown","metadata":{"id":"4DTk3ITq7NCr","colab_type":"text"},"source":["The accumulating functions are mostly the same in torch tensors as in numpy arrays.\n"]},{"cell_type":"code","metadata":{"id":"UrpBYCA77NKo","colab_type":"code","colab":{}},"source":["a = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mYIN_LJv7NNM","colab_type":"code","colab":{}},"source":["a_sum = a.sum()\n","print(f\"a.sum() = {a_sum}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qtQqPjXm8BjJ","colab_type":"text"},"source":["Similarily to numpy, you can specify the axis which you want to accumulate along: \n","\n","``axis=0`` for columns and ``axis=1`` for rows (as oppose to numpy)."]},{"cell_type":"code","metadata":{"id":"QOy3HRB28A4m","colab_type":"code","colab":{}},"source":["# the mean of each column in the tensor a:\n","a.mean(axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_NRl1wa8oOm","colab_type":"text"},"source":["A very important function to use after you have done an accumulationg operation is the ``torch.item`` function.\n","This function return a scalar from a tensor which contain only one item in it."]},{"cell_type":"code","metadata":{"id":"yCdHLI5E8oiR","colab_type":"code","colab":{}},"source":["x = torch.tensor([[1]])\n","print(x, \"\\n\")\n","\n","print(x.item())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSZU5uiVJdHK","colab_type":"text"},"source":["Unlike numpy, `torch.max` return both the maximal element in  a tensor and its index.\n","\n","Use `dim=1` to maximize over each row of the tensor columns and `dims=0` to maximize over each column of the tensor rows.\n","\n","Take a moment and think, why is it useful? Yoy will see in the upcoming chapters."]},{"cell_type":"code","metadata":{"id":"NzTGGCYbJdUE","colab_type":"code","colab":{}},"source":["x = torch.randn(3,5)\n","print(x, \"\\n\")\n","\n","value, index = torch.max(x, dim=1) \n","print(value)\n","print(index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_7-lvSU04P84","colab_type":"text"},"source":["### Numpy Bridge"]},{"cell_type":"markdown","metadata":{"id":"Dd1yfLYlaL2S","colab_type":"text"},"source":["Converting a torch Tensor to a numpy array and vice versa is a breeze. The torch Tensor and numpy array will share their underlying memory locations, and changing one will change the other."]},{"cell_type":"code","metadata":{"id":"m0FzZ-bRVixy","colab_type":"code","colab":{}},"source":["a = np.array([[1,2],[3,4]])\n","print(a, \"\\n\")\n","\n","tensor = torch.from_numpy(a)\n","print(tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kql-nhgs5f16","colab_type":"text"},"source":["Pay attenation that the numpy array and the torch tensor share the same memory,\n","if you change the numpy array you chang the tensor and vice versa."]},{"cell_type":"code","metadata":{"id":"siljsQ_15uF2","colab_type":"code","colab":{}},"source":["a[0,0] = 5\n","print(tensor, \"\\n\")\n","\n","tensor[0,0] = 6\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y2GkB-AoafXX","colab_type":"code","colab":{}},"source":["b = tensor.numpy()\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y0yYyIDi-DVy","colab_type":"text"},"source":["## Autograd: Automatic Differentiation"]},{"cell_type":"markdown","metadata":{"id":"4xkgH6MiOs5g","colab_type":"text"},"source":["Central to all neural networks in PyTorch is the ``autograd`` package. Let’s first briefly visit this.\n","\n","The autograd package provides automatic differentiation for all operations on Tensors. \n","\n","It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."]},{"cell_type":"markdown","metadata":{"id":"_Dd2RLo3QRp2","colab_type":"text"},"source":["``torch.Tensor``, which you saw above, is the central class of the package. \n","\n","If you set its attribute ``.requires_grad`` as ``True``, it starts to track all operations on it. \n","\n","When you finish your computation you can call ``.backward()`` and have all the gradients computed automatically. \n","\n","The gradient for this tensor will be accumulated into .grad attribute."]},{"cell_type":"markdown","metadata":{"id":"f9RTTP0CTDFc","colab_type":"text"},"source":["There’s one more class which is very important for the autograd implementation - a ``Function``.\n","\n","Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a ``.grad_fn`` attribute that references to a Function that has created the Tensor (except for Tensors created by the user - their ``grad_fn`` is ``None``).\n","\n","If you want to compute the derivatives, you can call ``.backward()`` on a Tensor. If Tensor is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to ``backward()``, however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape."]},{"cell_type":"code","metadata":{"id":"jNQhjE4y4xyd","colab_type":"code","colab":{}},"source":["x = torch.ones(2, 2, requires_grad=True)\n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DIA4YePZ4x2S","colab_type":"code","colab":{}},"source":["y = x + 2\n","print(y, \"\\n\")\n","\n","z = y * y * 3\n","print(z, \"\\n\")\n","\n","out = z.mean()\n","print(out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_E-J1QQeUzRW","colab_type":"text"},"source":["Let’s backprop now. "]},{"cell_type":"code","metadata":{"id":"r85e9PXCSIiH","colab_type":"code","colab":{}},"source":["out.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fv7Iks0oVIcp","colab_type":"text"},"source":["Print gradients d(out)/dx"]},{"cell_type":"code","metadata":{"id":"3HKyiBpdSIlN","colab_type":"code","colab":{}},"source":["print(x.grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UR5AThjyWzrC","colab_type":"text"},"source":["Calculating the gradients with respect to all of our parameters is time-consuming.\n","\n","Therefore, there are times when we don't want the gradients to be computed.\n","\n","There are 3 ways to stop the gradients calculations:"]},{"cell_type":"markdown","metadata":{"id":"9oqjap_IXSNE","colab_type":"text"},"source":["1. Updating the ``.requires_grad`` attribute as ``False``"]},{"cell_type":"code","metadata":{"id":"xYciquXkSIgr","colab_type":"code","colab":{}},"source":["a = torch.randn(3, requires_grad=True)\n","print(a)\n","print(a.requires_grad, \"\\n\")\n","\n","a.requires_grad_(False)\n","\n","print(a)\n","print(a.requires_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DB6nqFYKX8f9","colab_type":"text"},"source":["2. Call .detach()"]},{"cell_type":"code","metadata":{"id":"19A5z1J9SIaN","colab_type":"code","colab":{}},"source":["a = torch.randn(3, requires_grad=True)\n","print(a)\n","print(a.requires_grad, \"\\n\")\n","\n","b = a.detach()\n","\n","print(b)\n","print(b.requires_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RBVUsH9UY9ZR","colab_type":"text"},"source":["Pay attention that the gradients above share the same memory."]},{"cell_type":"markdown","metadata":{"id":"QqqRo3Y0ZFXT","colab_type":"text"},"source":["3. Using the ``with`` statement"]},{"cell_type":"code","metadata":{"id":"ZH77Bg30ZFoi","colab_type":"code","colab":{}},"source":["a = torch.randn(3, requires_grad=True)\n","print(a)\n","print(a.requires_grad, \"\\n\")\n","\n","with torch.no_grad():\n","  b = a+2\n","  print(b)\n","  print(b.requires_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Fj56RHmaQZ-","colab_type":"text"},"source":["As mentioned earlier, the gradients are accumulated in the ``.grad``  attribute.\n","\n","Let's see an example:"]},{"cell_type":"code","metadata":{"id":"9PT-pzsbarhF","colab_type":"code","colab":{}},"source":["weights = torch.ones(4, requires_grad=True)\n","\n","for epoch in range(3):\n","    model_output = (weights*3).sum()\n","    model_output.backward()\n","    \n","    print(weights.grad)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QzJBdvUKkcpU","colab_type":"text"},"source":["That isn't what we want.\n","\n","We must zero the gradients before each iteration."]},{"cell_type":"code","metadata":{"id":"pN1DfRa8arkH","colab_type":"code","colab":{}},"source":["weights = torch.ones(4, requires_grad=True)\n","\n","for epoch in range(3):\n","    model_output = (weights*3).sum()\n","    model_output.backward()\n","  \n","    print(weights.grad)\n","\n","    weights.grad.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"maZnGuwo-bEu","colab_type":"text"},"source":["##  Training Pipeline Overview: Model, Loss, and Optimizer"]},{"cell_type":"markdown","metadata":{"id":"LqWvVHn4B2Tm","colab_type":"text"},"source":["There are 3 common steps in order to make NN using pytorch.\n","1. Design model (input, output, forward pass with different layers).\n","2. Construct loss and optimizer.\n","3. Training loop:\n","      - Forward - compute prediction and loss.\n","      - Backward - compute gradients.\n","      - Update weights.\n","\n","Its important to understand the different components of the process and the well defined separation between them.\n","- Forward - only needs the X features, and the current weights.\n","- Backward - only needs the y label and the y_predicted to be given to the loss. So the gradients can be acumulated with respect to the loss (being calculated using ``loss.backprop()``).\n","- Update weights - only needs the gradients and the optimizing method."]},{"cell_type":"markdown","metadata":{"id":"ZOMMiUS8-iTA","colab_type":"text"},"source":["##  Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"PheQ9BoSoYvU","colab_type":"text"},"source":["Here is our first very simple example of NN.\n","\n","This NN implements the simple linear regression.\n","\n","The most important thing to take from this example is the structure of our code."]},{"cell_type":"markdown","metadata":{"id":"PCRTjGO5pO_r","colab_type":"text"},"source":["First, let's make a small dataset:"]},{"cell_type":"code","metadata":{"id":"cPSRp6zG_vk4","colab_type":"code","colab":{}},"source":["X_numpy, y_numpy = datasets.make_regression(n_samples=100, \n","                                            n_features=1, \n","                                            noise=15, \n","                                            random_state=42)\n","\n","X = torch.from_numpy(X_numpy.astype(np.float32))\n","y = torch.from_numpy(y_numpy.astype(np.float32))\n","y = y.view(y.shape[0], 1)\n","\n","n_samples, n_features = X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GCuQh5twsVqr","colab_type":"code","colab":{}},"source":["plt.scatter(X , y)\n","plt.title(\"Linear Regression Data\")\n","plt.xlabel(\"X\")\n","plt.ylabel(\"y\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"avnbhCM8DQLH","colab_type":"text"},"source":["Step No. 1:\n","\n","We define our NN as a class that inherite from ``nn.Module``.\n","\n","In this class we must implement two methods: the ``__init__`` method\n","and the ``forward`` method.\n","\n","In the ``__init__`` method we firstly calling ``nn.Module`` ``__init__`` and then we define our NN layers, their input and outputs sizes.\n","\n","The  ``forward`` method performs the forward propagtion using the layers defined in the initialization. Most layers are already implemented for you in the ``nn`` module of pytorch. the first layer to be used is a ``nn.Linear`` layer, defined by its input dimenstions and output dimensions, storing the wights inside. In order to apply the layer one need just to pass an X to it as if it was a function."]},{"cell_type":"code","metadata":{"id":"Y6x9Vt_nnsuE","colab_type":"code","colab":{}},"source":["class LinearRegression(nn.Module):\n","\n","  def __init__(self, input_dim, output_dim):\n","    super(LinearRegression, self).__init__()\n","\n","    self.lin = nn.Linear(input_dim, output_dim)\n","\n","  def forward(self, x):\n","    return self.lin(x)   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-xkf4Ywns2U","colab_type":"code","colab":{}},"source":["model = LinearRegression(n_features, 1)\n","model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0dZUiVhaav1k","colab_type":"text"},"source":["Model class was made so it will contain layers that operate in a sequence. As such, a very convineint way to reffer to all parameters of all layers is the method ``.parameters()``."]},{"cell_type":"code","metadata":{"id":"rHubETGnsyBt","colab_type":"code","colab":{}},"source":["w, b = model.parameters()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHwacilQtrG7","colab_type":"code","colab":{}},"source":["plt.plot(X, w.item()*X+b.item(), c=\"r\")\n","plt.scatter(X, y)\n","plt.title(\"Initial Model\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oe3E08-SGMAw","colab_type":"text"},"source":["Step No 2.\n","\n","We define our loss function from ``torch.nn`` and our optimizer from ``torch.optim``. \n","\n","Notice that as ``nn.Linear``, a loss function is just another layer function operating on a layer output - simply with no weights stored inside. which puts it in the ``torch.nn`` module.\n","\n","Also notice how an optimizer needs to accept the parameters on which to optimize on. In the example above we chose to include all parameters off the model in the optimizing process. In other scenarios, For example in transfer learning, we would want to limit the parameters to be optimizing on just the last layers and not change the first layers."]},{"cell_type":"code","metadata":{"id":"2xuF6BgSns1M","colab_type":"code","colab":{}},"source":["learning_rate = 0.01\n","n_iters = 100\n","\n","loss = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DLcVQdXEGom7","colab_type":"text"},"source":["Step No 3.\n","\n","Here we perform our training loop.\n","\n","- forward propagation.\n","- computing loss.\n","- back propagation.\n","- parameters update."]},{"cell_type":"code","metadata":{"id":"0WbHgpHMrFHP","colab_type":"code","colab":{}},"source":["for epoch in range(n_iters):\n","    # predict = forward pass with our model\n","    y_predicted = model(X)\n","\n","    # loss\n","    l = loss(y_predicted, y)\n","\n","    # calculate gradients = backward pass\n","    l.backward()\n","\n","    # update weights\n","    optimizer.step()\n","\n","    # zero the gradients after updating\n","    optimizer.zero_grad()\n","\n","    if (epoch+1) % 10 == 0:\n","        w, b = model.parameters()\n","        print('epoch ', epoch+1,' loss = ', l.item())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-HKQCRHrFKr","colab_type":"code","colab":{}},"source":["w, b = model.parameters()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6KJSWDh2rFNi","colab_type":"code","colab":{}},"source":["plt.plot(X, w.item()*X+b.item(), c=\"r\")\n","plt.scatter(X, y)\n","plt.title(\"Final Model\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-84Y2lymdYm0","colab_type":"text"},"source":["Try running the training again and examine what happened. Do you expect that to happen?"]},{"cell_type":"markdown","metadata":{"id":"c592nR1__vsZ","colab_type":"text"},"source":["## ✍️  Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"Vp61bcpnPGUj","colab_type":"text"},"source":["In this section, you ware going to implement logistic regression using pytorch."]},{"cell_type":"markdown","metadata":{"id":"cLqmXnU4N2j4","colab_type":"text"},"source":["A short reminder of logistic regression:\n","\n","Let  $W$ be our parametrs matrix and $b$ our bias.\n","\n","The logistic function is defined by the following formula of the sigmoid function:\n","\n","$\\sigma(x; W,b) = \\frac{1}{1+e^{-(W\\cdot x+b)}}$\n"]},{"cell_type":"markdown","metadata":{"id":"KnCugy93ekzB","colab_type":"text"},"source":["✍️ Step No. 0:\n","\n","Load the data and shape it so you can train a model on it."]},{"cell_type":"code","metadata":{"id":"qjCCelGB_104","colab_type":"code","colab":{}},"source":["bc = datasets.load_breast_cancer()\n","X, y = bc.data, bc.target\n","\n","n_samples, n_features = X.shape\n","\n","### ✍️ Split your data to train set and test set. Use 20% of your data as test.\n","### START CODE HERE ### (1 line of code)\n","\n","### END CODE HERE ###\n","\n","\n","### ✍️ Scale your data using the StandardScaler.\n","### START CODE HERE ### (~3 lines of code)\n","\n","### END CODE HERE ###\n","\n","### ✍️ Transform your data and labels to tensors. \n","### Don't forget to convert their dtype to float32.\n","### START CODE HERE ### (4 lines of code)\n","\n","### END CODE HERE ###\n","\n","### ✍️ Reshape your lables.\n","### START CODE HERE ### (2 lines of code)\n","\n","### END CODE HERE ###\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBe0LDX_k-7V","colab_type":"text"},"source":["✍️ Step No. 1:\n","\n","Define the LogisticRegression model. (read relevant layers in the nn documentaion)\n","\n","In the ``__init__`` method, define the layer and the activation.\n","\n","In the ``forward`` method, define forward propagartion.\n","\n","Finally, initiate a new LogisticRegression instance."]},{"cell_type":"code","metadata":{"id":"zBOujCq4kzWQ","colab_type":"code","colab":{}},"source":["class LogisticRegression(nn.Module):\n","    def __init__(self, n_input_features):\n","        super(LogisticRegression, self).__init__()\n","        ### START CODE HERE ### (2 lines of code)\n","\n","        ### END CODE HERE ###\n","\n","    def forward(self, x):\n","        ### START CODE HERE ### (~1-3 lines of code)\n","\n","        ### END CODE HERE ###\n","\n","### START CODE HERE ### (1 line of code)\n","\n","### END CODE HERE ###\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTApwLFLsRHC","colab_type":"text"},"source":["✍️ Step No 2.\n","\n","Now, we define our loss function as the binary cross-entropy loss,\n","\n","and stochastic gradient-descent as our optimization algorithm."]},{"cell_type":"code","metadata":{"id":"ctCEZoq_mmgz","colab_type":"code","colab":{}},"source":["num_epochs = 100\n","learning_rate = 0.01\n","\n","### START CODE HERE ### (2 lines of code)\n","\n","### END CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yYlGomS5tt7a","colab_type":"text"},"source":["✍️ Step No 3.\n","\n","Now, we perform our training loop.\n","\n","- forward propagation.\n","- computing loss.\n","- back propagation.\n","- parameters update."]},{"cell_type":"code","metadata":{"id":"VFz9m1-yYN1z","colab_type":"code","colab":{}},"source":["for epoch in range(num_epochs):\n","\n","  ### Forward pass\n","  ### START CODE HERE ### (1 line of code)\n","\n","  ### END CODE HERE ###\n","\n","  ### Loss calculation\n","  ### START CODE HERE ### (1 line of code)\n","\n","  ### END CODE HERE ###\n","\n","  ### Backward pass\n","  ### START CODE HERE ### (3 lines of code)\n","\n","  ### END CODE HERE ###\n","\n","  ### Print loss every 10 epochs\n","  ### START CODE HERE ### (2 lines of code)\n","\n","  ### END CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"svP4MtRy0RAp","colab_type":"text"},"source":["✍️ Finally, We must check how our model perforn on the test set.\n","\n","In this part we make a forward pass, however, we don't want the gradient to be calculated."]},{"cell_type":"code","metadata":{"id":"RGGrV4aSYOP6","colab_type":"code","colab":{}},"source":["### Compute the test accuracy. \n","### Use 0.5 as the classification border, samples with sigmoid result greater\n","### or equal to 0.5 will be classified as class 1, \n","### and all the others will be classifies as class 0.\n","### START CODE HERE ### (~5 lines of code)\n","\n","### END CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9lWfOt4J_3Va","colab_type":"text"},"source":["## Dataset and DataLoader"]},{"cell_type":"markdown","metadata":{"id":"C3RQHq5SHDyN","colab_type":"text"},"source":["As You probably saw in Andrew's course,\n","the gradient computation is not efficient for the whole data set.\n","\n","Therefore, we divide the dataset into small batches, and compute the gradient on each batch."]},{"cell_type":"markdown","metadata":{"id":"g3xcskOxHnP3","colab_type":"text"},"source":["A short reminder about the defenitions regarding this part:\n","- epoch - one forward and backward pass of ALL training samples.\n","- batch_size - number of training samples used in one forward/backward pass.\n","- number of iterations - number of passes, each pass (forward+backward) using #batch_size number of sampes.\n","\n","e.g : 100 samples, batch_size=20 -> 100/20=5 iterations for 1 epoch.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NRjFq-LvTREl","colab_type":"text"},"source":["As explained above it might be a good idea to an object representing a Dataset to so we can feed the model in training time.\n","\n","In order to implement such a Dataset, we will inherit the class ``Dataset`` of pytorch.\n","\n","We will implement the following methods: ``__init__``, ``__getitem__`` , and ``__len__``,\n","\n","using the scikit-learn wine dataset."]},{"cell_type":"code","metadata":{"id":"qUn0SOxd_66l","colab_type":"code","colab":{}},"source":["class WineDataset(Dataset):\n","\n","    def __init__(self):\n","        data = datasets.load_wine(return_X_y=True)\n","        self.n_samples = len(data[1])\n","\n","        self.x_data = torch.from_numpy(data[0])\n","        self.y_data = torch.from_numpy(data[1]) \n","\n","    # support indexing such that dataset[i] can be used to get i-th sample\n","    def __getitem__(self, index):\n","        return self.x_data[index], self.y_data[index]\n","\n","    # we can call len(dataset) to return the size\n","    def __len__(self):\n","        return self.n_samples\n","\n","\n","# create dataset\n","dataset = WineDataset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dfSBV-rXVYhD","colab_type":"text"},"source":["Now, let's see that we can get the first sample:"]},{"cell_type":"code","metadata":{"id":"ux3zQCxtVSg9","colab_type":"code","colab":{}},"source":["first_data = dataset[0]\n","features, labels = first_data\n","print(features, \"\\n\", labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sx4xARqVVCsH","colab_type":"text"},"source":["Once we defined our ``Dataset`` object we can use the already implemented ``DataLoader`` class wraping the ``Dataset`` object with batching skills and more.\n","\n","The main aim of the ``DataLoader`` is to provide us a convinient way to iterate through the batches of our dataset.\n","\n"]},{"cell_type":"code","metadata":{"id":"tSsyXPuZVC4u","colab_type":"code","colab":{}},"source":["data_loader = DataLoader(dataset=dataset,\n","                          batch_size=5,\n","                          shuffle=True,\n","                          num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ALJMBrR-bNRB","colab_type":"text"},"source":["We have just defined our data loader,\n","\n","now, let's convert it to an iterator and look at one random sample."]},{"cell_type":"code","metadata":{"id":"0EyzOR5KVC7l","colab_type":"code","colab":{}},"source":["dataiter = iter(data_loader)\n","data = dataiter.next()\n","features, labels = data\n","print(features, \"\\n\", labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SA1zs4OyXikV","colab_type":"text"},"source":["✍️ Iterate through the batches of the dataset.\n","For each batch print how much samples it has from each class."]},{"cell_type":"code","metadata":{"id":"EvjJQWFwXMQj","colab_type":"code","colab":{}},"source":["### START CODE HERE ### (~5 lines of code)\n","for i, (inputs, labels) in enumerate(data_loader):\n","  class_0 = sum(labels == 0).item()\n","  class_1 = sum(labels == 1).item()\n","  class_2 = sum(labels == 2).item()\n","\n","  print(f'In batch {i+1}: class 0 : {class_0},  class 1: {class_1},  class 2 : {class_2}')\n","### END CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tMPdPmMDZMZH","colab_type":"text"},"source":["This exercise purpose is to let you find on your own the best and most convinient way to write the ``for`` loop command that iterate througt the batches."]},{"cell_type":"markdown","metadata":{"id":"rWwEpMST_7c6","colab_type":"text"},"source":["## Transformers"]},{"cell_type":"markdown","metadata":{"id":"dGJQsNAiE2u4","colab_type":"text"},"source":["Transforms can be applied to PIL images, tensors, ndarrays, or custom data\n","during creation of the DataSet.\n","\n","You can find the complete list of built-in transforms here: \n","https://pytorch.org/docs/stable/torchvision/transforms.html\n","\n","Usually, transformers are sent while initializing the dataset as an optional paramrter.\n","Then the transform, if it exists, is being operated in the ``__getitem__`` method. \n","\n","In this way one can use transformers to delay some pre-process to realtime or make the pre-process depend on the batch itself. \n","But mainly this is used to do augmentaions that varies with every iteration, and has an intrinsic randomness. We will see some examples in this chapter."]},{"cell_type":"markdown","metadata":{"id":"DgBtHp2cE-aq","colab_type":"text"},"source":["Some common transforms which you can find implemented in ``torchvision.transforms``:\n","\n","\n","- On Images:\n","\n","> CenterCrop, Grayscale, Pad, RandomAffine, RandomCrop, RandomHorizontalFlip, RandomRotation, Resize, Scale.\n","\n","\n","\n","- On Tensors:\n","\n","> LinearTransformation, Normalize, RandomErasing.\n","\n","- Conversion:\n","\n","> ToPILImage: from tensor or ndrarray.\n","\n","> ToTensor : from numpy.ndarray or PILImage.\n","\n","- Custom:\n","\n","> Write your own class."]},{"cell_type":"code","metadata":{"id":"qpglC4AHbrvW","colab_type":"code","colab":{}},"source":["class LinearData(Dataset):\n","  \n","  def __init__(self, n_samples=100, transform=None):\n","    X,y = datasets.make_regression(n_samples=n_samples,\n","                                   n_features=1,\n","                                   noise=15,\n","                                   random_state=42)   \n","    self.x = X\n","    self.y = y.reshape((n_samples, -1))\n","    self.n_samples = n_samples\n","    self.transform = transform\n","\n","  def __getitem__(self, index):\n","    sample = self.x[index], self.y[index]\n","    if self.transform:\n","      sample = self.transform(sample)\n","    return sample\n","\n","  def __len__(self):\n","    return self.n_samples\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDyIX-J6Nd1w","colab_type":"code","colab":{}},"source":["class ToFloat32:\n","    def __call__(self, sample):\n","        inputs, targets = sample\n","        return inputs.astype(\"float32\"), targets.astype(\"float32\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-Kg-fdpPV4_","colab_type":"code","colab":{}},"source":["print('Without transform:')\n","dataset = LinearData(n_samples=500)\n","features, labels = dataset[0]\n","print(features.dtype, labels.dtype, \"\\n\")\n","\n","print('With transform to float32:')\n","dataset = LinearData(n_samples=500, transform=ToFloat32())\n","features, labels = dataset[0]\n","print(features.dtype, labels.dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6zp1hiSkOz2s","colab_type":"text"},"source":["✍️ Implement ToTensor transform.\n","\n","  Show that your transform works."]},{"cell_type":"code","metadata":{"id":"1BscvwvbMbw2","colab_type":"code","colab":{}},"source":["### START CODE HERE ### (~4 lines of code)\n","class ToTensor:\n","  def __call__(self, sample):\n","    inputs, targets = sample\n","    return torch.from_numpy(inputs), torch.from_numpy(targets)\n","### END CODE HERE ###  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uNBATEPxRsZe","colab_type":"code","colab":{}},"source":["### START CODE HERE ### (~8 lines of code)\n","print('Without transform:')\n","dataset = LinearData(n_samples=500)\n","features, labels = dataset[0]\n","print(type(features), type(labels), \"\\n\")\n","\n","print('With tensor transform:')\n","dataset = LinearData(n_samples=500, transform=ToTensor())\n","features, labels = dataset[0]\n","print(type(features), type(labels))\n","### END CODE HERE ###  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UF7QvPTgUSBO","colab_type":"text"},"source":["✍️ Implement MulTransform. This transformer have to get a factor and multiply the labels with this given factor."]},{"cell_type":"code","metadata":{"id":"pZoy8D8jtL55","colab_type":"code","colab":{}},"source":["### START CODE HERE ### (~7 lines of code)\n","class MulTransform:\n","    def __init__(self, factor):\n","        self.factor = factor\n","\n","    def __call__(self, sample):\n","        input, target = sample\n","        target *= self.factor\n","        return input, target\n","### END CODE HERE ###  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U4cjLfrBudsO","colab_type":"text"},"source":["✍️ Now, after we have built 3 transformers, let's compose them.\n","\n","Using ``torchvision.transforms`` documentation, write code which compose the above transformers.\n","\n","Your transform have to convert a numpy array to tensor of type float32 and multiply the labels of the data by 4. Show your code works. "]},{"cell_type":"code","metadata":{"id":"q5TSfEXzipM3","colab_type":"code","colab":{}},"source":["### START CODE HERE ### (~12 lines of code)\n","composed = torchvision.transforms.Compose([ToFloat32(), ToTensor(), MulTransform(4)])\n","plt.figure()\n","\n","for transform, title in zip([None, composed], [\"Without transform\", \"With transform\"]):\n","  dataset = LinearData(n_samples=500, transform=transform)\n","  xs , ys = [], []\n","  for i in range(len(dataset)):\n","    xs.append(dataset[i][0].item())\n","    ys.append(dataset[i][1].item())\n","  plt.scatter(xs, ys, label = title)\n","  \n","plt.legend()\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.title(\"Linear dataset\")\n","### END CODE HERE ###  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6PFQSPImABS5","colab_type":"text"},"source":["## ✍️ Feed-Forward Neural Network"]},{"cell_type":"markdown","metadata":{"id":"5BRD8u16Ilkw","colab_type":"text"},"source":["In this chapter you will write your first nueral network on your own!"]},{"cell_type":"markdown","metadata":{"id":"jCoAIHTZIwB4","colab_type":"text"},"source":["In this task we will use the well known MNIST dataset. Remember that this is a multiclass classification problem! \n","\n","Our nueral network will have 1 hidden layers."]},{"cell_type":"markdown","metadata":{"id":"kuyGTaAJKTpW","colab_type":"text"},"source":["At first, let's define some hyper-parameters:"]},{"cell_type":"code","metadata":{"id":"pF4N-TlMvl5m","colab_type":"code","colab":{}},"source":["input_size = 784 # 28x28\n","hidden_size = 240 \n","num_classes = 10\n","num_epochs = 2\n","batch_size = 100\n","learning_rate = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gMz8FcVRKfaF","colab_type":"text"},"source":["Now, we define our dataset and dataloader for both the train and test set:"]},{"cell_type":"code","metadata":{"id":"4nIJqHPsvM5Q","colab_type":"code","colab":{}},"source":["\n","train_dataset = torchvision.datasets.MNIST(root='./data', \n","                                           train=True, \n","                                          transform=torchvision.transforms.ToTensor(),  \n","                                           download=True)\n","\n","test_dataset = torchvision.datasets.MNIST(root='./data', \n","                                          train=False, \n","                                          transform=torchvision.transforms.ToTensor())\n","\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n","                                          batch_size=batch_size, \n","                                          shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ffhQDIHUKtS6","colab_type":"text"},"source":["✍️ Define your nueral network class. Use ReLU as your activation."]},{"cell_type":"code","metadata":{"id":"mN3Z5pljvdcH","colab_type":"code","colab":{}},"source":["### START CODE HERE ### (~12 lines of code)\n","\n","### END CODE HERE ###  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HQEJD-QGLJ_G","colab_type":"text"},"source":["✍️ Initialize your model with the above hyper-parameters. "]},{"cell_type":"code","metadata":{"id":"kMICh4D1FAqH","colab_type":"code","colab":{}},"source":["### START CODE HERE ### (~1 line of code)\n","\n","### END CODE HERE ###  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_ODwLF6Lc8v","colab_type":"text"},"source":["✍️ Define the loss (which one should you choose? check the documentation and read it deeply! Fix your model definition if needed.) and use SGD as your optimization algorithm."]},{"cell_type":"code","metadata":{"id":"r1_cW9A9FBxO","colab_type":"code","colab":{}},"source":["### START CODE HERE ### (2 lines of code)\n"," \n","### END CODE HERE ###  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c3CyAywlM-Ul","colab_type":"text"},"source":["✍️ Write the training loop:"]},{"cell_type":"code","metadata":{"id":"tOfQgs7_FGkO","colab_type":"code","colab":{}},"source":["start = time.time()\n","\n","### START CODE HERE ### (~10 lines of code)\n","\n","### END CODE HERE ### \n","\n","end = time.time()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6M2t9uLXTicx","colab_type":"code","colab":{}},"source":["print(f\"Training took {end-start} seconds.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xgLL8cXKNR4y","colab_type":"text"},"source":["✍️ Test your model on the test data. \n","\n","Make sure you don't waste time computing things you don't need! \n","\n","What is your accuracy?"]},{"cell_type":"code","metadata":{"id":"OnpRDSeDz4Qw","colab_type":"code","colab":{}},"source":["### START CODE HERE ### (~10 lines of code)\n","\n","### END CODE HERE ### "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6sNoppkeNoz_","colab_type":"text"},"source":["Our accuracy is much better than a random guess, however, we want to improve it!\n","\n","In order to improve our model preformence we should check whether our model overfits the data or try different hyper-parmeters.\n","\n","In order to check if our model overfits the data we can plot the loss of the training and the validation as function of the epoch number.\n","We will not get into it now, but you will do it in your next exercices!\n","\n","So, now we will try different hyper-parameters in order to achieve better preformance.\n","However, training took too much time! We must use our resources wisely!\n","In the next chapter we will learn how to use the gpu!"]},{"cell_type":"markdown","metadata":{"id":"Ql8lYhQbP4L6","colab_type":"text"},"source":["## Using the GPU"]},{"cell_type":"markdown","metadata":{"id":"tNM-yUGYZMzI","colab_type":"text"},"source":["By default all tensors are created on the CPU,\n","but you can also move them to the GPU (if it's available)."]},{"cell_type":"code","metadata":{"id":"4mxvlKWFP4aW","colab_type":"code","colab":{}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NzXMzvSbapzb","colab_type":"text"},"source":["You can directly create a tensor on GPU:"]},{"cell_type":"code","metadata":{"id":"35NiTndkZ95P","colab_type":"code","colab":{}},"source":["x = torch.rand(5,3) \n","print(x, \"\\n\")\n","y = torch.ones_like(x, device=device)\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qno4hdqla014","colab_type":"text"},"source":["Or you can just move it to the GPU. \n","\n","Additional option to indicate device is to use strings ``.to(\"cuda\")``"]},{"cell_type":"code","metadata":{"id":"bpVBH4s9Z-CO","colab_type":"code","colab":{}},"source":["x = x.to(device)\n","print(x)                      "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-JVsJlffbO2g","colab_type":"text"},"source":["It's not possible to convert tensor which is on the GPU to a numpy array because numpy cannot handle GPU tenors."]},{"cell_type":"code","metadata":{"id":"3UHHQDBkbDwX","colab_type":"code","colab":{}},"source":["z = x + y\n","print(z)\n","z = z.numpy() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M89kdQ1vcTzl","colab_type":"text"},"source":["You have to move the tensor to the CPU and then convert it to a numpy array.\n"]},{"cell_type":"code","metadata":{"id":"mURj8_DcbL9Z","colab_type":"code","colab":{}},"source":["z = z.to(\"cpu\")       \n","z = z.numpy()\n","print(z)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tl_9etX3nR9K","colab_type":"text"},"source":["Now, after you have learnt how to run tensors on the GPU let's try it!\n","\n","✍️ Try to run your model from the previous chapter on the GPU.\n","In order to make it you have to move any inputs/tensors and your model to the GPU.\n","\n","Did your time performance got better?"]},{"cell_type":"markdown","metadata":{"id":"ZEKIgsEAvST-","colab_type":"text"},"source":["\n","\n","\n","\n","Probably, your answer is no.\n","\n","This can happen when the cost of transferring data between RAM and GPU memory is more than the speedup of parallel computation on the GPU.\n","\n","It can happen when your model is quite small, or in case when you have too many transfers of data in your forward() function.\n","\n","If you still want to see improvment in the time preformance, increase the number of epochs and use only 1 batch (batch = all your dataset) "]},{"cell_type":"markdown","metadata":{"id":"1sE06VviAKiR","colab_type":"text"},"source":["## RNNs"]},{"cell_type":"code","metadata":{"id":"OQljClGBJjZj","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}